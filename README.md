# MLOps Platform POC

This project is a Proof of Concept (POC) for an MLOps platform utilizing MLFlow for tracking and deployment, and MLServer on Seldon-Core for inference serving.

## Hands on serving ML models

Want to get your hands dirty? Follow our step-by-step guide to serve your first ML model using this platform:
[➡️ Hands-On: Serving ML Models](./docs/basic-hands-on/README.md)

## Overview

The platform is designed to streamline the machine learning lifecycle, from experimentation and model training to deployment and serving in a Kubernetes environment.

Key components and technologies used:

*   **MLFlow:** For experiment tracking, model versioning, and model packaging.
*   **Seldon-Core:** For deploying machine learning models on Kubernetes.
*   **MLServer:** As the inference server for serving models, supporting both HTTP and gRPC.
*   **Minikube:** For local Kubernetes cluster deployment.
*   **MinIO:** As an S3-compatible object storage for MLFlow artifacts.
*   **XGBoost:** Used for training a benchmark model.
*   **Helm:** For packaging and deploying applications on Kubernetes.

## Project Structure

```
.
├── benchmarks/                     # Scripts for training benchmark models (e.g., XGBoost)
├── docker/                         # Dockerfiles for custom images (e.g., MLServer image)
├── k8s-manifests/                  # Kubernetes manifests (e.g., SeldonDeployment CRDs, MinIO deployment)
├── model-deployments/              # Configuration for model deployments
├── helm-charts/                    # Helm charts for deploying components (e.g., MLFlow, custom applications)
└── .gitignore
```

## Features

*   **Experiment Tracking:** Leverages MLFlow to log parameters, metrics, and artifacts for each experiment.
    *   *(Placeholder for MLFlow UI Screenshot)*
*   **Model Registry:** Uses MLFlow Model Registry for versioning and managing model lifecycle.
*   **Automated Deployment:**
    *   Models are tracked and registered in MLFlow, with artifacts stored in MinIO (S3).
    *   Seldon-Core is used to deploy these models into a Minikube Kubernetes cluster.
*   **Inference Serving:**
    *   MLServer serves the deployed models, providing both HTTP and gRPC endpoints.
    *   *(Placeholder for MLServer API Spec/Swagger UI Screenshot)*
*   **Scalability:** SeldonDeployment supports Horizontal Pod Autoscaling (HPA).
*   **Custom Model Images:** Utilizes custom Docker images for serving models with MLServer.

## Setup and Deployment

### Prerequisites

*   Minikube
*   kubectl
*   Helm
*   Docker (or a compatible container runtime)

### Steps

1.  **Start Minikube:**
    ```bash
    minikube start
    ```

2.  **Deploy MLFlow (with bundled MinIO & PostgreSQL):**
    *   Navigate to the MLFlow Helm chart directory (e.g., `helm-charts/mlflow`).
    *   The MLFlow chart should handle the deployment of MinIO (for S3 artifact storage) and PostgreSQL (as the backend store).
    *   Review the chart's `values.yaml` to ensure internal MinIO and PostgreSQL are enabled and configure them if necessary (e.g., persistence, resource limits).
    *   Deploy MLFlow using Helm. If the chart handles internal S3 and database, you might not need to specify all S3 parameters explicitly, or they might refer to the chart's internal services. Consult the chart's documentation for specific configuration. An example command might still look like:
    ```bash
    helm install mlflow <chart-path> \
      --set backendStore.s3.accessKey=<YOUR_MINIO_ACCESS_KEY_IF_CUSTOM_OR_DEFAULT> \
      --set backendStore.s3.secretKey=<YOUR_MINIO_SECRET_KEY_IF_CUSTOM_OR_DEFAULT> \
      --set backendStore.s3.endpoint=<MLFLOW_CHART_MINIO_ENDPOINT_OR_EXTERNAL>
    ```
    *   Ensure you note down any access credentials and service endpoints if they are generated by the chart or if you've set them.

3.  **Install Seldon-Core Operator:**
    ```bash
    helm install seldon-core seldon-core-operator \
        --namespace seldon-system --create-namespace \
        --set usageMetrics.enabled=false \
        --repo https://storage.googleapis.com/seldon-charts
    ```

4.  **Train and Register a Model:**
    *   Run the training script located in the `benchmarks/` directory (e.g., `python benchmarks/train_xgb.py`).
    *   Ensure the script is configured to use the deployed MLFlow instance for tracking and model registration.

5.  **Build Custom MLServer Image (if applicable):**
    *   If you have custom model serving logic, navigate to the `docker/` directory.
    *   Build and push your custom MLServer image to a container registry accessible by Minikube (e.g., Docker Hub, or use Minikube's Docker daemon).
    ```bash
    # To use Minikube's Docker daemon
    eval $(minikube docker-env)
    docker build -t your-custom-mlserver-image:latest docker/your-mlserver-dockerfile-dir
    ```

6.  **Deploy Model with SeldonDeployment:**
    *   Create or update a `SeldonDeployment` Custom Resource Definition (CRD) YAML file (examples might be in `model-deployments/` or `k8s-manifests/`).
    *   This CRD will reference the MLFlow model URI (from MinIO) or your custom MLServer image.
    *   Example `SeldonDeployment` structure:
        ```yaml
        apiVersion: machinelearning.seldon.io/v1
        kind: SeldonDeployment
        metadata:
          name: my-xgb-model
          namespace: default # Or your target namespace
        spec:
          name: xgboost-iris
          predictors:
            - componentSpecs:
                - spec:
                    containers:
                      - name: classifier
                        image: <your-custom-mlserver-image:latest> # Or a pre-built MLServer image
                        env:
                          - name: MODEL_URI # If using MLFlow model from S3
                            value: "s3://mlflow/..." # Path to your model in MinIO
                          - name: MLSERVER_HTTP_PORT
                            value: "9000"
                          - name: MLSERVER_GRPC_PORT
                            value: "9500"
                        ports:
                          - containerPort: 9000
                            name: http
                          - containerPort: 9500
                            name: grpc
              graph:
                name: classifier
                type: MODEL
                endpoint:
                  type: REST # Can also be GRPC
              name: default
              replicas: 1 # Configure HPA later if needed
        ```
    *   Apply the SeldonDeployment:
        ```bash
        kubectl apply -f your-seldon-deployment.yaml
        ```

7.  **Accessing the Served Model:**
    *   Once the SeldonDeployment is ready, Seldon creates a Kubernetes service.
    *   Forward the service port to access it locally:
        ```bash
        # For HTTP
        kubectl port-forward svc/<seldon-deployment-name>-<predictor-name>-<graph-name> 8080:8000 # Adjust ports as needed

        # For gRPC
        # kubectl port-forward svc/<seldon-deployment-name>-<predictor-name>-<graph-name> 8081:8081
        ```
    *   Send inference requests to `http://localhost:8080/v2/models/classifier/infer` (or the gRPC equivalent).
    *   *(Placeholder for example `curl` request or client code snippet)*

## MLServer API

*   *(Placeholder for MLServer API Spec - Link to Swagger/OpenAPI definition or embedded spec)*
*   **HTTP Endpoint:** `/v2/models/<model-name>/infer`
*   **gRPC Endpoint:** Details for gRPC service and methods.

## Advanced Features

*   **Horizontal Pod Autoscaling (HPA):** Configure HPA for SeldonDeployments based on metrics.
*   **MLServer Features:** Explore advanced MLServer features like multi-model serving, adaptive batching, etc.


## TODO
*   **Helm** Clean y provide full usable templates. (TODO)
*   **Integrate with ArgoCD:** Automate GitOps deployment pipeline using ArgoCD. (TODO)
*   **Monitoring and Logging:** Set up monitoring and logging for the deployed models and infrastructure. (TODO)
